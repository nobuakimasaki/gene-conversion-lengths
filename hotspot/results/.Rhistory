})
})
# Check for NAs
na_counts <- map_dfr(vars, function(v) {
tibble(
variable = v,
na_count = sum(is.na(res_boot[[v]])),
total = length(res_boot[[v]]),
percent_na = mean(is.na(res_boot[[v]])) * 100
)
})
# Merge with point estimates
res_df <- inner_join(long_est, quantiles_by_model_type, by = c("model", "variable", "type"))
6.364658e+02*9.750000e-03 + 2.135565e+01*(1-9.750000e-03)
library(dplyr)
library(stringr)
# Get all CSV file names in the directory
files <- list.files(pattern = "^fit.*\\.csv$", full.names = TRUE)
# Read and row-bind all CSV files
results <- do.call(rbind, lapply(files, read.csv)) %>%
group_by(distribution, model, iteration) %>%
slice_min(AIC, with_ties = FALSE)
# analyzing bootstrap
# # Get all CSV file paths from the "bootstrap" subdirectory
# files <- list.files("bootstrap", pattern = "\\.csv$", full.names = TRUE)
#
# # Read and combine all CSV files
# boot_res <- do.call(rbind, lapply(files, read.csv))
#
# boot_res <- boot_res %>%
#   group_by(distribution, model, iteration, bootstrap_iter) %>%
#   slice_min(AIC, with_ties = FALSE)
#
# write.csv(boot_res, "combined_boot_res.csv")
boot_res <- read.csv("combined_boot_res.csv")
# Define the columns you want bootstrap intervals for
cols <- c("w1", "NLL", "AIC", "mean", "phi1", "phi2")
# Compute 2.5% and 97.5% quantiles for each column within each group
boot_percentiles <- boot_res %>%
group_by(model, distribution, iteration) %>%
summarise(across(all_of(cols),
list(lower = ~quantile(.x, 0.025, na.rm = TRUE),
upper = ~quantile(.x, 0.975, na.rm = TRUE)),
.names = "{.col}_{.fn}"),
.groups = "drop")
boot_percentiles <- boot_percentiles %>%
mutate(mean_covers_100 = (mean_lower <= 100 & mean_upper >= 100))
combined <- left_join(results, boot_percentiles, by = c("model", "distribution", "iteration"))
AIC_selected <- combined %>%
group_by(distribution, iteration) %>%
slice_min(AIC, with_ties = FALSE) %>%
ungroup() %>%
mutate(model = "AIC-selected")
combined_augmented <- bind_rows(combined, AIC_selected)
library(dplyr)
library(stringr)
# Get all CSV file names in the directory
files <- list.files(pattern = "^fit.*\\.csv$", full.names = TRUE)
# Read and row-bind all CSV files
results <- do.call(rbind, lapply(files, read.csv)) %>%
group_by(distribution, model, iteration) %>%
slice_min(AIC, with_ties = FALSE)
# analyzing bootstrap
# # Get all CSV file paths from the "bootstrap" subdirectory
# files <- list.files("bootstrap", pattern = "\\.csv$", full.names = TRUE)
#
# # Read and combine all CSV files
# boot_res <- do.call(rbind, lapply(files, read.csv))
#
# boot_res <- boot_res %>%
#   group_by(distribution, model, iteration, bootstrap_iter) %>%
#   slice_min(AIC, with_ties = FALSE)
#
# write.csv(boot_res, "combined_boot_res.csv")
boot_res <- read.csv("combined_boot_res.csv")
# Define the columns you want bootstrap intervals for
cols <- c("w1", "NLL", "AIC", "mean", "phi1", "phi2")
# Compute 2.5% and 97.5% quantiles for each column within each group
boot_percentiles <- boot_res %>%
group_by(model, distribution, iteration) %>%
summarise(across(all_of(cols),
list(lower = ~quantile(.x, 0.025, na.rm = TRUE),
upper = ~quantile(.x, 0.975, na.rm = TRUE)),
.names = "{.col}_{.fn}"),
.groups = "drop")
boot_percentiles <- boot_percentiles %>%
mutate(mean_covers_100 = (mean_lower <= 100 & mean_upper >= 100))
combined <- left_join(results, boot_percentiles, by = c("model", "distribution", "iteration"))
AIC_selected <- combined %>%
group_by(distribution, iteration) %>%
slice_min(AIC, with_ties = FALSE) %>%
ungroup() %>%
mutate(model = "AIC-selected")
combined_augmented <- bind_rows(combined, AIC_selected)
View(combined_augmented)
library(dplyr)
library(stringr)
# Get all CSV file names in the directory
files <- list.files(pattern = "^fit.*\\.csv$", full.names = TRUE)
# Read and row-bind all CSV files
results <- do.call(rbind, lapply(files, read.csv)) %>%
group_by(distribution, model, iteration) %>%
slice_min(AIC, with_ties = FALSE)
# analyzing bootstrap
# # Get all CSV file paths from the "bootstrap" subdirectory
# files <- list.files("bootstrap", pattern = "\\.csv$", full.names = TRUE)
#
# # Read and combine all CSV files
# boot_res <- do.call(rbind, lapply(files, read.csv))
#
# boot_res <- boot_res %>%
#   group_by(distribution, model, iteration, bootstrap_iter) %>%
#   slice_min(AIC, with_ties = FALSE)
#
# write.csv(boot_res, "combined_boot_res.csv")
boot_res <- read.csv("combined_boot_res.csv")
# Define the columns you want bootstrap intervals for
cols <- c("w1", "NLL", "AIC", "mean", "phi1", "phi2")
# Compute 2.5% and 97.5% quantiles for each column within each group
boot_percentiles <- boot_res %>%
group_by(model, distribution, iteration) %>%
summarise(across(all_of(cols),
list(lower = ~quantile(.x, 0.025, na.rm = TRUE),
upper = ~quantile(.x, 0.975, na.rm = TRUE)),
.names = "{.col}_{.fn}"),
.groups = "drop")
boot_percentiles <- boot_percentiles %>%
mutate(mean_covers_100 = (mean_lower <= 100 & mean_upper >= 100))
combined <- left_join(results, boot_percentiles, by = c("model", "distribution", "iteration"))
AIC_selected <- combined %>%
group_by(distribution, iteration) %>%
slice_min(AIC, with_ties = FALSE) %>%
ungroup() %>%
mutate(model = "AIC-selected")
combined_augmented <- bind_rows(combined, AIC_selected)
View(results)
View(combined_augmented)
View(AIC_selected)
View(combined)
library(dplyr)
library(purrr)
library(tibble)
library(readr)
library(tidyr)
library(stringr)
library(ggplot2)
# loading in tract information used for estimation
l_psi_hotspot <- read.csv("l_psi_hotspot.csv")
l_psi_nonhotspot <- read.csv("l_psi_nonhotspot.csv")
# number of tracts
nrow(l_psi_hotspot)
nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot)/(nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot))
# results for geometric model
res_null_hotspot <- read.csv("res_null_hotspot.csv")
res_null_hotspot$type <- "hotspot"
res_null_nonhotspot <- read.csv("res_null_nonhotspot.csv")
res_null_nonhotspot$type <- "nonhotspot"
# results for sum of geometric RVs model
res_sum_hotspot <- read.csv("res_sum_hotspot.csv")
res_sum_hotspot$type <- "hotspot"
res_sum_nonhotspot <- read.csv("res_sum_nonhotspot.csv")
res_sum_nonhotspot$type <- "nonhotspot"
# results for mixture model, get row with lowest NLL
res_mixture_hotspot <- read.csv("res_mixture_hotspot.csv") %>% arrange(NLL)
res_mixture_nonhotspot <- read.csv("res_mixture_nonhotspot.csv") %>% arrange(NLL)
res_mixture_hotspot <- res_mixture_hotspot[1,]
res_mixture_hotspot$type <- "hotspot"
res_mixture_nonhotspot <- res_mixture_nonhotspot[1,]
res_mixture_nonhotspot$type <- "nonhotspot"
res_df <- rbind(res_null_hotspot, res_null_nonhotspot,
res_sum_hotspot, res_sum_nonhotspot,
res_mixture_hotspot, res_mixture_nonhotspot)
res_df$mean1 <- 1/res_df$phi1
res_df$mean2 <- 1/res_df$phi2
# change to long format
long_est <- res_df %>%
select(-X) %>%  # drop X column
pivot_longer(
cols = c(NLL, AIC, mean, w1, phi1, phi2, mean1, mean2),
names_to = "variable",
values_to = "est")
# filter to hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_hotspot <- long_est %>% filter(type == "hotspot")
long_est_hotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_hotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# filter to non-hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_nonhotspot <- long_est %>% filter(type == "nonhotspot")
long_est_nonhotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_nonhotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# load all bootstrapped estimates
file_list <- list.files(pattern = "^bootstrap_res_.*hotspot_seed[0-9]+.*\\.csv$", full.names = TRUE)
res_list <- lapply(file_list, function(file) {
df <- read.csv(file)
# Extract seed number
seed <- as.integer(str_match(basename(file), "seed(\\d+)")[,2])
# Extract type: hotspot or nonhotspot
type <- ifelse(str_detect(file, "nonhotspot"), "nonhotspot", "hotspot")
# Add seed and type columns
df$seed <- seed
df$type <- type
# Add mean1 and mean2
df$mean1 <- 1 / df$phi1
df$mean2 <- 1 / df$phi2
return(df)
})
# For each seed, model, and for hotspot tracts and non-hotspot tracts, obtain bootstrapped estimate
res_boot <- do.call(rbind, res_list) %>%
group_by(model, bootstrap, seed, type) %>%
slice_min(NLL, with_ties = FALSE) %>%
ungroup() %>%
arrange(model, seed, bootstrap)
# Assign a new index for each bootstrap, seed combination. This is one boostrap iteration.
res_boot <- res_boot %>%
group_by(seed, bootstrap) %>%
mutate(index = group_indices()) %>%
ungroup()
# There should be 42*12*3*2 = 3024 rows (looks fine)
# Checking individual models
res_boot_null <- res_boot %>% filter(model == "null")
res_boot_sum <- res_boot %>% filter(model == "sum")
res_boot_mixture <- res_boot %>% filter(model == "mixture")
# limit to 500 bootstrap iterations
res_boot <- filter(res_boot, index <= 500)
# Compute quantiles grouped by model and type
q_probs <- c(0.025, 0.975)
vars <- c("NLL", "AIC", "w1", "phi1", "phi2", "mean1", "mean2", "mean")
# Get quantiles for 95% CIs
quantiles_by_model_type <- res_boot %>%
group_by(model, type) %>%
group_split() %>%
map_dfr(function(subdf) {
model_val <- unique(subdf$model)
type_val <- unique(subdf$type)
map_dfr(vars, function(v) {
q_vals <- quantile(subdf[[v]], probs = q_probs, na.rm = TRUE)
tibble(
model = model_val,
type = type_val,
variable = v,
q2.5 = q_vals[[1]],
q97.5 = q_vals[[2]]
)
})
})
# Check for NAs
na_counts <- map_dfr(vars, function(v) {
tibble(
variable = v,
na_count = sum(is.na(res_boot[[v]])),
total = length(res_boot[[v]]),
percent_na = mean(is.na(res_boot[[v]])) * 100
)
})
# Merge with point estimates
res_df <- inner_join(long_est, quantiles_by_model_type, by = c("model", "variable", "type"))
View(res_df)
3.991104e+06
4.012714e+06
3.901982e+06
View(long_est_hotspot)
View(l_psi_nonhotspot)
View(l_psi_nonhotspot)
View(long_est_nonhotspot)
8.204464e+06
8.248436e+06
7.950875e+06
long_est_hotspot <- long_est %>% filter(type == "hotspot")
long_est_hotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_hotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# filter to non-hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_nonhotspot <- long_est %>% filter(type == "nonhotspot")
long_est_nonhotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_nonhotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
View(res_df)
library(dplyr)
library(purrr)
library(tibble)
library(readr)
library(tidyr)
library(stringr)
library(ggplot2)
# loading in tract information used for estimation
l_psi_hotspot <- read.csv("l_psi_hotspot.csv")
l_psi_nonhotspot <- read.csv("l_psi_nonhotspot.csv")
# number of tracts
nrow(l_psi_hotspot)
nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot)/(nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot))
# results for geometric model
res_null_hotspot <- read.csv("res_null_hotspot.csv")
res_null_hotspot$type <- "hotspot"
res_null_nonhotspot <- read.csv("res_null_nonhotspot.csv")
res_null_nonhotspot$type <- "nonhotspot"
# results for sum of geometric RVs model
res_sum_hotspot <- read.csv("res_sum_hotspot.csv")
res_sum_hotspot$type <- "hotspot"
res_sum_nonhotspot <- read.csv("res_sum_nonhotspot.csv")
res_sum_nonhotspot$type <- "nonhotspot"
# results for mixture model, get row with lowest NLL
res_mixture_hotspot <- read.csv("res_mixture_hotspot.csv") %>% arrange(NLL)
res_mixture_nonhotspot <- read.csv("res_mixture_nonhotspot.csv") %>% arrange(NLL)
res_mixture_hotspot <- res_mixture_hotspot[1,]
res_mixture_hotspot$type <- "hotspot"
res_mixture_nonhotspot <- res_mixture_nonhotspot[1,]
res_mixture_nonhotspot$type <- "nonhotspot"
res_df <- rbind(res_null_hotspot, res_null_nonhotspot,
res_sum_hotspot, res_sum_nonhotspot,
res_mixture_hotspot, res_mixture_nonhotspot)
res_df$mean1 <- 1/res_df$phi1
res_df$mean2 <- 1/res_df$phi2
# change to long format
long_est <- res_df %>%
select(-X) %>%  # drop X column
pivot_longer(
cols = c(NLL, AIC, mean, w1, phi1, phi2, mean1, mean2),
names_to = "variable",
values_to = "est")
# filter to hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_hotspot <- long_est %>% filter(type == "hotspot")
long_est_hotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_hotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# filter to non-hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_nonhotspot <- long_est %>% filter(type == "nonhotspot")
long_est_nonhotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_nonhotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# load all bootstrapped estimates
file_list <- list.files(pattern = "^bootstrap_res_.*hotspot_seed[0-9]+.*\\.csv$", full.names = TRUE)
res_list <- lapply(file_list, function(file) {
df <- read.csv(file)
# Extract seed number
seed <- as.integer(str_match(basename(file), "seed(\\d+)")[,2])
# Extract type: hotspot or nonhotspot
type <- ifelse(str_detect(file, "nonhotspot"), "nonhotspot", "hotspot")
# Add seed and type columns
df$seed <- seed
df$type <- type
# Add mean1 and mean2
df$mean1 <- 1 / df$phi1
df$mean2 <- 1 / df$phi2
return(df)
})
# For each seed, model, and for hotspot tracts and non-hotspot tracts, obtain bootstrapped estimate
res_boot <- do.call(rbind, res_list) %>%
group_by(model, bootstrap, seed, type) %>%
slice_min(NLL, with_ties = FALSE) %>%
ungroup() %>%
arrange(model, seed, bootstrap)
# Assign a new index for each bootstrap, seed combination. This is one boostrap iteration.
res_boot <- res_boot %>%
group_by(seed, bootstrap) %>%
mutate(index = group_indices()) %>%
ungroup()
View(res_df)
# There should be 42*12*3*2 = 3024 rows (looks fine)
# Checking individual models
res_boot_null <- res_boot %>% filter(model == "null")
res_boot_sum <- res_boot %>% filter(model == "sum")
res_boot_mixture <- res_boot %>% filter(model == "mixture")
# limit to 500 bootstrap iterations
res_boot <- filter(res_boot, index <= 500)
# Compute quantiles grouped by model and type
q_probs <- c(0.025, 0.975)
vars <- c("NLL", "AIC", "w1", "phi1", "phi2", "mean1", "mean2", "mean")
# Get quantiles for 95% CIs
quantiles_by_model_type <- res_boot %>%
group_by(model, type) %>%
group_split() %>%
map_dfr(function(subdf) {
model_val <- unique(subdf$model)
type_val <- unique(subdf$type)
map_dfr(vars, function(v) {
q_vals <- quantile(subdf[[v]], probs = q_probs, na.rm = TRUE)
tibble(
model = model_val,
type = type_val,
variable = v,
q2.5 = q_vals[[1]],
q97.5 = q_vals[[2]]
)
})
})
# Check for NAs
na_counts <- map_dfr(vars, function(v) {
tibble(
variable = v,
na_count = sum(is.na(res_boot[[v]])),
total = length(res_boot[[v]]),
percent_na = mean(is.na(res_boot[[v]])) * 100
)
})
# Merge with point estimates
res_df <- inner_join(long_est, quantiles_by_model_type, by = c("model", "variable", "type"))
library(dplyr)
library(purrr)
library(tibble)
library(readr)
library(tidyr)
library(stringr)
library(ggplot2)
# loading in tract information used for estimation
l_psi_hotspot <- read.csv("l_psi_hotspot.csv")
l_psi_nonhotspot <- read.csv("l_psi_nonhotspot.csv")
# number of tracts
nrow(l_psi_hotspot)
nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot)
nrow(l_psi_hotspot)/(nrow(l_psi_hotspot) + nrow(l_psi_nonhotspot))
# results for geometric model
res_null_hotspot <- read.csv("res_null_hotspot.csv")
res_null_hotspot$type <- "hotspot"
res_null_nonhotspot <- read.csv("res_null_nonhotspot.csv")
res_null_nonhotspot$type <- "nonhotspot"
# results for sum of geometric RVs model
res_sum_hotspot <- read.csv("res_sum_hotspot.csv")
res_sum_hotspot$type <- "hotspot"
res_sum_nonhotspot <- read.csv("res_sum_nonhotspot.csv")
res_sum_nonhotspot$type <- "nonhotspot"
# results for mixture model, get row with lowest NLL
res_mixture_hotspot <- read.csv("res_mixture_hotspot.csv") %>% arrange(NLL)
res_mixture_nonhotspot <- read.csv("res_mixture_nonhotspot.csv") %>% arrange(NLL)
res_mixture_hotspot <- res_mixture_hotspot[1,]
res_mixture_hotspot$type <- "hotspot"
res_mixture_nonhotspot <- res_mixture_nonhotspot[1,]
res_mixture_nonhotspot$type <- "nonhotspot"
res_df <- rbind(res_null_hotspot, res_null_nonhotspot,
res_sum_hotspot, res_sum_nonhotspot,
res_mixture_hotspot, res_mixture_nonhotspot)
res_df$mean1 <- 1/res_df$phi1
res_df$mean2 <- 1/res_df$phi2
# change to long format
long_est <- res_df %>%
select(-X) %>%  # drop X column
pivot_longer(
cols = c(NLL, AIC, mean, w1, phi1, phi2, mean1, mean2),
names_to = "variable",
values_to = "est")
# filter to hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_hotspot <- long_est %>% filter(type == "hotspot")
long_est_hotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_hotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# filter to non-hotspot estimates using the mixture model, and obtain AIC
# obtain difference in AIC between the mixture model and geometric model
long_est_nonhotspot <- long_est %>% filter(type == "nonhotspot")
long_est_nonhotspot %>% filter(model == "mixture" & variable == "AIC") %>% select(est) - long_est_nonhotspot %>% filter(model == "null" & variable == "AIC") %>% select(est)
# load all bootstrapped estimates
file_list <- list.files(pattern = "^bootstrap_res_.*hotspot_seed[0-9]+.*\\.csv$", full.names = TRUE)
res_list <- lapply(file_list, function(file) {
df <- read.csv(file)
# Extract seed number
seed <- as.integer(str_match(basename(file), "seed(\\d+)")[,2])
# Extract type: hotspot or nonhotspot
type <- ifelse(str_detect(file, "nonhotspot"), "nonhotspot", "hotspot")
# Add seed and type columns
df$seed <- seed
df$type <- type
# Add mean1 and mean2
df$mean1 <- 1 / df$phi1
df$mean2 <- 1 / df$phi2
return(df)
})
# For each seed, model, and for hotspot tracts and non-hotspot tracts, obtain bootstrapped estimate
res_boot <- do.call(rbind, res_list) %>%
group_by(model, bootstrap, seed, type) %>%
slice_min(NLL, with_ties = FALSE) %>%
ungroup() %>%
arrange(model, seed, bootstrap)
# Assign a new index for each bootstrap, seed combination. This is one boostrap iteration.
res_boot <- res_boot %>%
group_by(seed, bootstrap) %>%
mutate(index = group_indices()) %>%
ungroup()
# There should be 42*12*3*2 = 3024 rows (looks fine)
# Checking individual models
res_boot_null <- res_boot %>% filter(model == "null")
res_boot_sum <- res_boot %>% filter(model == "sum")
res_boot_mixture <- res_boot %>% filter(model == "mixture")
# limit to 500 bootstrap iterations
res_boot <- filter(res_boot, index <= 500)
# Compute quantiles grouped by model and type
q_probs <- c(0.025, 0.975)
vars <- c("NLL", "AIC", "w1", "phi1", "phi2", "mean1", "mean2", "mean")
# Get quantiles for 95% CIs
quantiles_by_model_type <- res_boot %>%
group_by(model, type) %>%
group_split() %>%
map_dfr(function(subdf) {
model_val <- unique(subdf$model)
type_val <- unique(subdf$type)
map_dfr(vars, function(v) {
q_vals <- quantile(subdf[[v]], probs = q_probs, na.rm = TRUE)
tibble(
model = model_val,
type = type_val,
variable = v,
q2.5 = q_vals[[1]],
q97.5 = q_vals[[2]]
)
})
})
# Check for NAs
na_counts <- map_dfr(vars, function(v) {
tibble(
variable = v,
na_count = sum(is.na(res_boot[[v]])),
total = length(res_boot[[v]]),
percent_na = mean(is.na(res_boot[[v]])) * 100
)
})
# Merge with point estimates
res_df <- inner_join(long_est, quantiles_by_model_type, by = c("model", "variable", "type"))
